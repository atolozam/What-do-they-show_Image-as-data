{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtener noticias a través de Google**\n",
    "_Por Juan Carlos Rodríguez-Raga y Andrés Mauricio Toloza Cruz_\n",
    "\n",
    "Este código proporciona herramientas a través de Selenium y Newspaper3k para hacer web scrapping a noticias que aparezcan en la pestaña de noticias de Google. \n",
    "\n",
    "Este trabajo se hace en el marco del proyecto \"What do they show?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Primer paso: descargar e importar nuestras librerias**\n",
    "Las librerias que estaremos usando son Selenium y Newspaper3k  para descargarlas usamos el siguiente código:\n",
    "\n",
    "    ! pip install selenium\n",
    "\n",
    "    ! pip install newspaper3k\n",
    "\n",
    "Tambien usaremos time y urllib, pero esas ya vienen en nuestra instalación de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las lbrerias necesarias\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import newspaper\n",
    "from newspaper import ArticleException\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Segundo paso: extraer nuestras noticias de Google**\n",
    "\n",
    "Para este paso es importante conocer nuestro buscador, en este caso Google. Para este ejemplo quiero descargar las noticias que hablen de protestas en los siguientes medios digitales de Colombia, esto de 2011 a 2024. \n",
    "\n",
    "La lista de esos medios y su link enlace es la siguiente:\n",
    "\n",
    "| Nombre | Enlace |\n",
    "| ------ | ------ |\n",
    "| El Tiempo | eltiempo.com |\n",
    "| El Espectador | elespectador.com |\n",
    "| Semana | semana.com |\n",
    "| El Colombiano | elcolombiano.com |\n",
    "| El Pais Colombia | elpais.com.co |\n",
    "| Caracol Radio | caracol.com.co |\n",
    "| Portafolio | portafolio.com |\n",
    "| Vanguardia | vanguardia.com |\n",
    "| Minuto 30 | minuto30.com |\n",
    "| Noticias RCN | noticiasrcn.com |\n",
    "| Las dos Orillas | las2orillas.co |\n",
    "| La Silla Vacia | lasillavacia.com |\n",
    "| RCN Radio | rcn.com.co |\n",
    "| W Radio | wradio.com.co |\n",
    "| Dinero | dinero.com | \n",
    "| La Republica | larepublica.com |\n",
    "\n",
    "Saber el enlace raiz es importante porque este nos va a permitir hacer busquedas en Google más especificas, para esto vamos a utilizar _[site:enlace raiz]_. En el caso del tiempo Google nos da el filtro _before:fecha_ y _after:fecha_. Hay que tener en cuenta que si Google cree que no encuentra resultados suficientes va a saltarse el filtro de tiempo.\n",
    "\n",
    "Por ejemplo, si solo me interesará mirar protestas entre 2019 y 2022 podria hacer lo siguiente:\n",
    "\n",
    "_protestas colombia after:2018-12-31 before:2023-01-01 [site:enlace raiz]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un diccionario con nuestros enaleces raiz y un diccionario para los resultados\n",
    "\n",
    "bases = [\n",
    "        'eltiempo.com', 'elespectador.com', 'semana.com', 'elcolombiano.com',\n",
    "         'elpais.com.co', 'caracol.com.co', 'portafolio.com', 'vanguardia.com',\n",
    "         'minuto30.com', 'noticiasrcn.com', 'las2orillas.co', 'lasillavacia.com',\n",
    "         'rcn.com.co', 'larepublica.com', 'dinero.com', 'wradio.com.co'\n",
    "         ]\n",
    "\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto ya comenzamos a usar la libreria de Selenium, la cual nos permite ejecutar tareas automatizadas de webscrapping. En este caso, le doy de input el link de resultado de mi busqueda de Google y alteramos ciertos parametros para que puede alternar entre páginas de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino mi función scroll que permite hacer un scroll en la página web para cargar más noticias\n",
    "\n",
    "def scroll():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ciclo es el que permite extraer los links de la noticia a través del XPATH del elemento HTML que contiene el link de esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in bases:\n",
    "    for pag in range(220, 301, 10):\n",
    "        links = ['https://www.google.com/search?q=protestas+colombia+after:2019-12-31+before:2025-01-01+%5Bsite:'+str(base)+'%5D&sca_esv=a0ce93ff23945b86&rlz=1C5CHFA_enCO941CO941&tbm=nws&ei=f41UZuTZE4DJ1sQPhcSegAg&start='+str(pag)+'&sa=N&ved=2ahUKEwikldDg-q2GAxWApJUCHQWiB4AQ8tMDegQIAxAE&biw=1200&bih=846&dpr=1'\n",
    "        ]\n",
    "        for link in links:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(link)\n",
    "            time.sleep(3)\n",
    "\n",
    "            errores=0\n",
    "            for j in range(2, 12):\n",
    "                \n",
    "                try:\n",
    "                    scroll()\n",
    "                    img_element = driver.find_element(By.XPATH, '//*[@id=\"rso\"]/div/div/div['+str(j)+']/div/div/a')\n",
    "                    # Agrego el link a la lista\n",
    "                    resultados.append(img_element.get_attribute('href'))\n",
    "                    print('Link added, iteration ' + str(j-1) + ' base: ' + str(base))\n",
    "                except NoSuchElementException:\n",
    "                    print('No image found for this iteration ('+ str(j) +'), moving to the next one.')\n",
    "                    errores=errores+1\n",
    "                    if errores == 2:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                time.sleep(5)\n",
    "                driver.quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hago un checkpoint de los links obtenidos\n",
    "\n",
    "import csv\n",
    "# Abre (o crea) un archivo CSV en modo escritura ('w')\n",
    "with open('links_checkpoint_news.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Escribe el título como la primera fila en el archivo CSV\n",
    "    writer.writerow(['Links'])\n",
    "    # Escribe cada enlace como una fila en el archivo CSV\n",
    "    for link in resultados:\n",
    "        # El método writerow espera una lista, así que ponemos el enlace en una lista\n",
    "        writer.writerow([link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tercer paso: extraer la información de los links y hacer pequeñas limpiezas**\n",
    "\n",
    "En este último paso aplicaré pequeños filtros para limpiar mi lista de links y extraer la mayor cantidad de información con la libreria de Newspaper3k. Para finalmente filtrar noicias que mencionen palabras relacionadas con Protesta, Paro, Marcha, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lee el archivo CSV\n",
    "df_news = pd.read_csv('links_checkpoint_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las filas duplicadas\n",
    "\n",
    "df_news = df_news.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pongo las filas de manera aleatoria (esto ayuda a tener mejores resultados con Newspaper3k)\n",
    "\n",
    "df_news = df_news.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un diccionario para guardar los resultados\n",
    "\n",
    "parse_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizo la extracción de noticias con Newspaper3k\n",
    "\n",
    "import newspaper\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from newspaper import ArticleException\n",
    "import json\n",
    "\n",
    "num = 1\n",
    "\n",
    "for url in df_news.iloc['Links']:\n",
    "\n",
    "    try: \n",
    "\n",
    "        article = newspaper.Article(url=url, language='es')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        article_dict ={\n",
    "            \"title\": str(article.title),\n",
    "            \"published_date\": str(article.publish_date),\n",
    "            \"top_image\": str(article.top_image),\n",
    "            \"link\": urlparse(url).netloc,\n",
    "            \"full_link\": str(url),\n",
    "            \"text\": str(article.text)\n",
    "        }\n",
    "\n",
    "        parse_data[article_dict[\"title\"]] = article_dict\n",
    "\n",
    "        print(str(num) + \" - \" + str(article_dict[\"title\"]))\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        num += 1\n",
    "\n",
    "    except ArticleException:\n",
    "        article_dict ={\n",
    "            \"title\": \"Error\",\n",
    "            \"published_date\": \"Error\",\n",
    "            \"top_image\": \"Error\",\n",
    "            \"link\": urlparse(url).netloc,\n",
    "            \"full_link\": str(url),\n",
    "            \"text\": \"Error\"\n",
    "        }\n",
    "\n",
    "        parse_data[article_dict[\"title\"]] = article_dict\n",
    "\n",
    "        print(str(num) + \" - \" + str(article_dict[\"title\"]))\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        num += 1\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierto mis resultados en un DataFrame\n",
    "\n",
    "df_extraidos = pd.DataFrame(list(parse_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporto el dataframe a un archivo CSV\n",
    "df_extraidos.to_csv('parse_data_chekpoint.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino mi lista de palabras para el primer filtro\n",
    "\n",
    "palabras = [\n",
    "        'Protesta', 'Protestas', 'protesta', 'protestas',\n",
    "        'Manifestaciones', 'Manifestación', 'manifestaciones', 'manifestación', 'Manifestacion', 'manifestacion',\n",
    "        'Marcha', 'Marchas', 'marcha', 'marchas',\n",
    "        'Paro', 'paro'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el filtro con las plabras\n",
    "\n",
    "palabras_regex = '|'.join(palabras)  \n",
    "df_filtro = df_extraidos[df_extraidos['text'].str.contains(palabras_regex, case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el resultado en un archivo CSV\n",
    "\n",
    "df_filtro.to_csv('parse_data_filtro1_chekpoint.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newspaper3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
